{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42fcc3d4-e4e4-41fd-8946-81b5cdd459b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import load_numpy_data\n",
    "from plot_utils import plot_diffusion_output\n",
    "from sail_pytorch_utils import instantiate_diffusion_dataloader\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f418a8-bb3a-4c85-8b6c-a20297baae18",
   "metadata": {},
   "source": [
    "# 0. Define configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41f2af1-544e-4c30-afca-1fddb9b3d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 96  # the generated image resolution\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # how many images to sample during evaluation\n",
    "    num_epochs = 50\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 2\n",
    "    save_model_epochs = 4\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"diffusion_from_training_amp_phase_data\"  # the model name locally and on the HF Hub\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    hub_model_id = \"dcarm/TestModel\"  # the name of the repository to create on the HF Hub\n",
    "    hub_private_repo = False\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd9914-16ff-43f9-bbe5-a3f0969fbe8f",
   "metadata": {},
   "source": [
    "# 1. Create noise predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2bd13dc-7077-4a3e-ae8e-7067241bde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=2,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=2,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(32, 64, 128),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d65dd-21e5-4b35-824a-5bbe63389c45",
   "metadata": {},
   "source": [
    "# 2. Create noise scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "708e251c-251c-4e91-8b03-8448c31730b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63bb39e-a283-4b7d-80a8-0c8a97ade09e",
   "metadata": {},
   "source": [
    "# 3. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a9cd1f2-be22-4ba1-973f-92350343770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = instantiate_diffusion_dataloader(config.train_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a57b4c1-6891-4273-b60c-36752084a943",
   "metadata": {},
   "source": [
    "# 4. Define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca96ce7c-d181-4d57-916a-9f8e962ee061",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cosine_schedule_with_warmup\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[1;32m      4\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m get_cosine_schedule_with_warmup(\n\u001b[1;32m      5\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m      6\u001b[0m     num_warmup_steps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlr_warmup_steps,\n\u001b[0;32m----> 7\u001b[0m     num_training_steps\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m config\u001b[38;5;241m.\u001b[39mnum_epochs),\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_data_loader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ba9dd-7c32-4d01-a55d-5a63e694d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "from diffusers.utils import make_image_grid\n",
    "import os\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.manual_seed(config.seed),\n",
    "    ).images\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    image_grid = make_image_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sail-pytorch-env",
   "language": "python",
   "name": "sail-pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
